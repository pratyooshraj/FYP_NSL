Step 1: Image acquisition
1.0: run video_to_image: this converts all videos with single letters to folder of images--done
1.1: run combo_video to image: this captures each letter from a single video and stores it in their respective directory as per user key input
1.2: capture_image: capture images and store in directory of vowel name

Step 2: Move files to respective locations using move_files.py

Step 3: Preprocessing
3.1: augment
3.2: train_test_val_split
3.3: annotate_hands_eff: needs to be done in batches, memory and cpu intensive

Step 4: Create YOLO data
4.1: run move_yolo_files: to move images/annotations from sub-directory of letter to train/test/val folder
4.2: create a master folder which is divided into train test val folder, each folders are then further divided to labels and annotations folder
4.3: create yaml file

Step 5: Train in google colab
5.1: add wanddb api_key
5.2: download the weights folder under runs, best.pt is the required model
5.3: clone yolov5 library and add the .pt file to it

Step 6: Create UI
6.1:
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124 for pytorch with cuda 12.4 support
6.2: Create UI and integrate model into it
6.3: Before running make sure you check the cache folder
6.4: Refactor

Enjoy!!!
All Rights Reserved